<p><strong>Collaborators</strong>: None<br />
<strong>Notes</strong>: All <span class="math inline">\(\log\)</span>s are base <span class="math inline">\(e\)</span>.<br />
</p>
<h1 class="unnumbered" id="problem-1-pac-learnability-for-axis-aligned-rectangles">Problem 1: PAC Learnability for Axis-Aligned Rectangles</h1>
<ol type="a">
<li><p>(<span>3pts</span>)</p>
<p><span id="sol:1a" label="sol:1a">[sol:1a]</span></p>
<p>To show that <span class="math inline">\(A\)</span> is ERM, it suffices to show that <span class="math inline">\(\hat{R}_S \left( A(S) \right) = \hat{R}_S \left( h^* \right) = 0\)</span> for any <span class="math inline">\(S\)</span>. Let <span class="math inline">\(B_{S}\)</span> be the tightest rectangle containing all positive examples from sample <span class="math inline">\(S\)</span>. By definition, we know that <span class="math inline">\(A(S)\)</span> returns decisions based on rectangle <span class="math inline">\(B_S\)</span> as <span class="math display">\[A(S; x_1, x_2) = \mathds{1}{\left\{(x_1, x_2) \in B_S\right\}}\]</span></p>
<p>We begin by claiming a stronger statement about <span class="math inline">\(B_S\)</span>: it contains <em>only</em> positive examples from <span class="math inline">\(S\)</span>. This follows from realizability: there exists true classifier <span class="math inline">\(h^* \in \cal H\)</span> which decides along rectangle <span class="math inline">\(B^*\)</span> as <span class="math display">\[\forall (x_1, x_2) \in \mathbb{R}^2, y_i = h^*(x_1, x_2) = \mathds{1}{\left\{(x_1, x_2) \in B^*\right\}}.\]</span> Therefore, since <span class="math inline">\(B_S\)</span> is bounded by positive examples through construction, <span class="math inline">\(B_S \subset B^*\)</span> and <span class="math inline">\(h^*\)</span> labels no negative examples in <span class="math inline">\(B_S\)</span>.</p>
<p>Since there are <em>only</em> positive examples in <span class="math inline">\(B_S\)</span>, every negative example in <span class="math inline">\(S\)</span> lies in <span class="math inline">\(B_S^c\)</span>. So, <span class="math inline">\(A(S)\)</span> classifies all points in <span class="math inline">\(S\)</span> correctly and <span class="math inline">\(\hat{R}_S \left( A(S) \right) = 0\)</span> follows.</p></li>
<li><p>(<span>5pts</span>)</p>
<p><span id="sol:1b" label="sol:1b">[sol:1b]</span> Recall from the reasoning in Solution  that <span class="math inline">\(B_S \subset B^*\)</span>. So, if <span class="math inline">\(B_\varepsilon := B^* - B_S\)</span> and <span class="math inline">\(P:\mathbb{R}^2 \rightarrow [0, 1]\)</span> is the data distribution, then <span class="math display">\[\begin{aligned}
            R\left(A(S) \right) &amp;= \mathbb{E}_{(x_1, x_2) \sim P} [\mathds{1}{\left\{A(S;x_1, x_2) \neq h^*(x_1, x_2)\right\}}]\\
                      &amp;= \mathbb{E}_{(x_1, x_2)\sim P} [\mathds{1}{\left\{(x_1, x_2) \in B_\varepsilon\right\}}] = P(B_\varepsilon)
        \end{aligned}\]</span> because <span class="math inline">\(A(S)\)</span> and <span class="math inline">\(h^*\)</span> only disagree on the set difference between the rectangles. Therefore, we’d like to show that sufficient sampling will expand <span class="math inline">\(B_S \rightarrow B^*\)</span> such that the mass of <span class="math inline">\(B_\varepsilon &lt; \varepsilon\)</span> with high probability.</p>
<p>Let <span class="math inline">\(R_1\)</span>, <span class="math inline">\(R_2\)</span>, <span class="math inline">\(R_3\)</span>, <span class="math inline">\(R_4\)</span> be the regions of <span class="math inline">\(B_\varepsilon\)</span> to the left, right, up, and down of <span class="math inline">\(B_S\)</span>, parallel to the closest similar axis of the region. Because <span class="math inline">\(R_1 \cup R_2 \cup R_3 \cup R_4 = B_\varepsilon\)</span>, we will be satisfied if <span class="math inline">\(P(R_i) \leq \varepsilon/4\)</span>. Additionally, to conclude the result <span class="math inline">\(P(B_\varepsilon) \leq \varepsilon\)</span> wp <span class="math inline">\(1-\delta\)</span>, each statement must be shown wp <span class="math inline">\(1-\delta/4\)</span> by union bound: <span class="math display">\[\begin{aligned}
            \Pr \{ P(R_1 \cap R_2 \cap R_3 \cap R_4) \leq \varepsilon\} &amp;= 1 - \Pr \{ P(R_1^c \cup R_2^c \cup R_3^c \cup R_4^c) \leq \varepsilon\} &amp;&amp; \textbf{(DeMorgan)} \\
            &amp;\geq 1 - \sum_{i = 1}^4 \Pr \{ P(R_i) \leq \varepsilon/4 \} &amp;&amp; \textbf{(union bound)}\\
            &amp;\geq 1 - 4 \cdot \delta/4. &amp;&amp; (\Pr \{ R_i &gt; \varepsilon/4 \} &lt; \delta/4)
        \end{aligned}\]</span> WLOG, consider <span class="math inline">\(R_1\)</span> and suppose that <span class="math inline">\(P(R_1) &gt; \varepsilon/4\)</span>. By the construction of our empirical risk minimizer <span class="math inline">\(A\)</span> and the fact that <span class="math inline">\(B_S\)</span> and <span class="math inline">\(R_1\)</span> are disjoint, no point could have been sampled in <span class="math inline">\(R_1\)</span>. (If a point were sampled from there, say, it would be revealed as a positive example and <span class="math inline">\(B_S\)</span> would extend into <span class="math inline">\(R_1\)</span> to include it). The probability of this happening despite <span class="math inline">\(n\)</span> i.i.d. samples is: <span class="math display">\[\begin{aligned}
            \Pr\{\text{no sample selected from } R_1 \text{ in } n \text{ draws}\} &amp;= \prod_{i = 1}^n \Pr\{\text{draw } i \notin R_1\} &amp;&amp; \textbf{(independence)} \\
            &amp;\leq \prod_{i =1}^n \left(1 - \frac{\varepsilon}{4}\right)^n &amp;&amp;  (\textbf{assumption } P(R_1) &gt; \varepsilon/4) \\
            &amp;\leq \exp(-\varepsilon n/4) &amp;&amp; (1-x \leq e^{-x})
        \end{aligned}\]</span> Since we’d like to bound this error probability by <span class="math inline">\(\delta/4\)</span>, we rearrange <span class="math display">\[\exp(-\varepsilon n/4) &lt; \delta/4 \Rightarrow n \geq \left\lceil\frac{4\log(4/\delta)}{\varepsilon}\right\rceil.\]</span> Since this sample lower bound holds for all <span class="math inline">\(R_i\)</span> simultaneously, it is sufficient to keep <span class="math inline">\(B_\varepsilon \leq \varepsilon\)</span> wp <span class="math inline">\(1 - \delta\)</span> and we are done.</p></li>
<li><p>(<span>5pts</span>)</p>
<p><span id="sol:1c" label="sol:1c">[sol:1c]</span> Once again, recall the strategy of Solution . We created equally-weighted ’buffers’ that occupied the area outward in the direction orthogonal to each edge of the rectangle <span class="math inline">\(B_S\)</span>. Generalizing this strategy to rectangles in dimension <span class="math inline">\(2d\)</span>, we first realize that there are now <span class="math inline">\(2d\)</span> total edges, two (parallel) segments in each dimension, and therefore <span class="math inline">\(4d\)</span> buffers to be created. This checks out with <span class="math inline">\(d = 2\)</span> where we saw <span class="math inline">\(4\)</span> regions. Accordingly, each of these regions maximum weight <span class="math inline">\(\varepsilon/(2d)\)</span> that must be guaranteed wp <span class="math inline">\(1 - \delta/(2d)\)</span>. So, by identical logic to Solution , we would require <span class="math display">\[n \geq \left\lceil\frac{2d \log (2d/\delta)}{\varepsilon}\right\rceil\]</span> samples.</p></li>
<li><p><span id="sol:1d" label="sol:1d">[sol:1d]</span> We first derive the complexity of <span class="math inline">\(A\)</span> wrt the input samples <span class="math inline">\(n\)</span>, which we know to be polynomial in <span class="math inline">\(d,1/\varepsilon\)</span> and <span class="math inline">\(\log(1/\delta)\)</span>. In the quest to find the ’tightest’ rectangle enclosing positive examples for each dimension <span class="math inline">\(i \in [d]\)</span>, <span class="math inline">\(A\)</span> must keep track of the smallest and largest positive example for each dimension, which can be done in one pass per dimension for total complexity <span class="math inline">\(\ensuremath{\mathcal{O}}(nd)\)</span> by counting comparisons. The total complexity of <span class="math inline">\(A\)</span> is therefore, <span class="math display">\[\ensuremath{\mathcal{O}}(nd) = \ensuremath{\mathcal{O}}\left(\frac{2d^2\log(2d/\delta)}{\varepsilon}\right) = \ensuremath{\mathcal{O}}(\mathop{\mathrm{poly}}(d, 1/\varepsilon, \log(1/\delta)))\]</span> and we are done.</p></li>
</ol>
<h1 class="unnumbered" id="problem-2-uniform-convergence-for-parameterized-hypothesis-classes">Problem 2: Uniform Convergence for Parameterized Hypothesis Classes</h1>
<ol type="a">
<li><p>(<span>2pts</span>)</p>
<p>It might be reasonable to expect loss functions of neural networks to behave in this manner (or any model using gradient-descent). If we consider <span class="math inline">\(\theta\)</span> to parametrize the weights and biases of network <span class="math inline">\(f\)</span>, our network maps <span class="math inline">\(x \mapsto f(\theta; x)\)</span> and might iterate using a regression loss <span class="math inline">\(\ell(h_\theta(x), y) = (h_\theta(x) - y)^2\)</span>. With our usual definitions of <span class="math inline">\(R\)</span> and <span class="math inline">\(\hat R\)</span>, we might like similar <span class="math inline">\(\theta\)</span>, <span class="math inline">\(\theta&#39;\)</span> parameters to result in similar <span class="math inline">\(R\)</span> and <span class="math inline">\(\hat R\)</span>. This is guaranteed in <span class="math inline">\(L^2\)</span> distance up to constants <span class="math inline">\(c_1, c_2\)</span> by <span class="math inline">\(L\)</span>-lipschitz functions. In other words: <span class="math display">\[\| \theta - \theta&#39; \|_2 &lt; a \Rightarrow |R(h_\theta) - R(h_{\theta&#39;})| &lt; c_1a \land |\hat{R}(h_\theta) - \hat{R}(h_{\theta&#39;})| &lt; c_2a\]</span> where <span class="math inline">\(a, b, c_1, c_2 &lt; \infty\)</span>.</p></li>
<li><p>(<span>5pts</span>)</p>
<p><span id="sol:2b" label="sol:2b">[sol:2b]</span> Let <span class="math inline">\({\bf v}_i\)</span> be a unit vector in the direction of the <span class="math inline">\(i\)</span>-th coordinate of vector <span class="math inline">\(\theta\)</span>. We assume that <span class="math inline">\({\bf v}_i\)</span> and <span class="math inline">\({\bf v}_j\)</span> are orthogonal if <span class="math inline">\(i \neq j\)</span>, so the collection surely spans <span class="math inline">\(\mathbb{R}^d\)</span>. Therefore, <span class="math display">\[S = \left \{\langle {\bf c}, {\bf v} \rangle = \sum_{i = 1}^d c_i {\bf v}_i , \hspace{0.5em}\|{\bf c}\|_2 \leq B \right\}\]</span> since the construction spans the subset of <span class="math inline">\(\mathbb{R}^d\)</span> with maximum <span class="math inline">\(L^2\)</span> norm of <span class="math inline">\(B\)</span>: a <span class="math inline">\(B\)</span>-sphere. On each direction <span class="math inline">\({\bf v}_i\)</span>, it is also easy to see that <span class="math inline">\(C_i\)</span>, the collection of <span class="math inline">\(i\)</span>-th coordinates of points in <span class="math inline">\(C\)</span>, discretizes segment <span class="math inline">\(\ell_i = [-B, B]{\bf v}_i\)</span> through gaps of <span class="math inline">\(\varepsilon/\sqrt d\)</span>. So, for any point in <span class="math inline">\(\ell\)</span>, there is always some <span class="math inline">\(c\in C_i\)</span> is <em>at most</em> <span class="math inline">\(\varepsilon/\sqrt d\)</span> from any point in <span class="math inline">\(\ell\)</span>. Therefore, for any <span class="math inline">\(\theta \in S\)</span>, there is some <span class="math inline">\(\theta&#39; \in C\)</span> with <span class="math display">\[\begin{aligned}
            \| \theta - \theta&#39; \|_2^2 &amp;= \left\| \sum_{i = 1}^d (c_i - d_i) {\bf v}_i \right\|_2^2, \hspace{2 em} \text{ where } d_i \in C_i,  \forall i \\
            &amp;\leq \sum_{i = 1}^d \| (c_i - d_i) {\bf v}_i\|_2^2 &amp;&amp; \textbf{(convexity of a norm)} \\
            &amp;\leq \sum_{i = 1}^d \left\| \frac{\varepsilon{\bf v}_i}{\sqrt d} \right\|_2^2 = d \cdot \varepsilon^2/d = \varepsilon^2, &amp;&amp; \exists d_i : c_i - d_i \leq \varepsilon/\sqrt d
        \end{aligned}\]</span> The desired result <span class="math inline">\(\|\theta - \theta&#39;\|_2 \leq \varepsilon\)</span> immediately follows.</p></li>
<li><p>(<span>3pts</span>)</p>
<p><span id="sol:2c" label="sol:2c">[sol:2c]</span> Suppose that <span class="math inline">\(B \sqrt d &lt; \varepsilon &lt; 3B\sqrt d\)</span>. Then, for each <span class="math inline">\(i \in [d]\)</span> (dimension), we have <span class="math inline">\(|j| \leq \frac{B\sqrt d}{\varepsilon} &lt; 1\)</span>, so we only have the point <span class="math inline">\(\theta_i = 0\)</span>, <span class="math inline">\(\forall \; i\)</span>. This is just the origin of <span class="math inline">\(\mathbb{R}^d\)</span>, so <span class="math display">\[|C| = 1 \leq \left(\frac{3B\sqrt d}{\varepsilon}\right)^d,\]</span> as desired.</p>
<p>Now we consider the case of <span class="math inline">\(0 &lt; \varepsilon \leq B\sqrt d\)</span>. Recall the definition of <span class="math inline">\(C_i\)</span> from Solution . Clearly, <span class="math inline">\(C \subseteq \prod_i C_i\)</span> so, <span class="math inline">\(|C| \leq \prod_i |C_i|\)</span>. We can simply count the maximum <span class="math inline">\(j\)</span> (i.e., ignore the <span class="math inline">\(C\subset S\)</span> condition) as <span class="math display">\[\underset{{j &lt; 0, j &gt; 0}}{\underbrace{2 \left\lfloor \frac{B\sqrt d}{\varepsilon}\right\rfloor}} + \underset{j = 0}{\underbrace{1}} \leq 3\left\lfloor \frac{B\sqrt d}{\varepsilon}\right\rfloor,\]</span> for each dimension <span class="math inline">\(i \in [d]\)</span>. The result follows: <span class="math display">\[\begin{aligned}
            |C| \leq \prod_i |C_i| &amp;\leq \left( 3\left\lfloor\frac{B\sqrt d}{\varepsilon} \right\rfloor\right)^d \leq \left( \frac{3B\sqrt d}{\varepsilon}\right)^d.
        \end{aligned}\]</span></p></li>
<li><p>(<span>2pts</span>)</p>
<p><span id="sol:2d" label="sol:2d">[sol:2d]</span> Recall Hoeffding’s inequality as <span class="math display">\[\Pr \left[\left|\frac 1n \sum_{i = 1}^n x_i - \mathbb E\left[ \frac 1n \sum_{i = 1}^n x_i\right]\right| &gt; \varepsilon\right] \leq 2 \exp\left(\frac{-2n^2 \varepsilon^2}{\sum_{i=1}^n (b_i - a_i)^2}\right) \label{eq:hoeffding}\]</span> where <span class="math inline">\(X_i\)</span> are independent with <span class="math inline">\(X_i \in [a, b]\)</span> a.s. for each <span class="math inline">\(i \in [n]\)</span>. We gently tailor the inequality for our purposes and finish with a union bound over the finite set.</p>
<p>Let random variable <span class="math inline">\(C_i = (x_i,y_i)\)</span> be drawn from the underlying data distribution <span class="math inline">\(P\)</span> and define <span class="math inline">\(X_{i, \theta} = \ell(h_\theta(x_i), y_i)\)</span>. It is easy to see that <span class="math inline">\(X_{i, \theta}\)</span> is also a random variable as <span class="math inline">\(\ell(h_\theta; C)\)</span> is a (measurable) function of random variable <span class="math inline">\(C\)</span>. So, for a given <span class="math inline">\(\theta \in C\)</span>, if we let <span class="math inline">\(x_{i, \theta} \sim X_{i, \theta}\)</span>, then: <span class="math display">\[\begin{aligned}
            \Pr \left[\left|\hat{R}(h_\theta) - \mathbb E\left[\hat{R}(h_{\theta})\right]\right| &gt; \alpha\right] &amp;\leq 2 \exp\left(\frac{-2n^2 \alpha^2}{(2M)^2 n}\right) &amp;&amp; (\forall i, b_i - a_i \leq 2M) \\
            \Rightarrow \Pr \left[\left|\hat{R}(h_\theta) - R(h_\theta)\right| &gt; \alpha\right] &amp;\leq 2 \exp\left(\frac{-n \alpha^2}{2M^2}\right). &amp;&amp; (\mathbb E [\hat R] = R) \\
        \end{aligned}\]</span> For this to hold for each <span class="math inline">\(|C|&lt; \infty\)</span> members, the standard union bound argument follows and we have <span class="math display">\[\Pr \left[\left|\hat{R}(h_\theta) - R(h_\theta)\right| \leq \alpha\right] \geq 1 - 2|C| \exp\left(\frac{-n \alpha^2}{2M^2}\right), \hspace{2 em} \forall \theta \in C\]</span></p></li>
<li><p>(<span>5pts</span>)</p>
<p><span id="sol:2e" label="sol:2e">[sol:2e]</span> By the triangle inequality, <span class="math display">\[\label{eq:triangle}
            |\hat{R}(h_\theta) - R(h_\theta)| \leq |\hat{R}(h_\theta) - \hat{R}(h_{\theta&#39;})| + |\hat{R}(h_{\theta&#39;}) - R(h_{\theta&#39;})| + |R(h_{\theta&#39;}) - R(h_\theta)|,\]</span> where <span class="math inline">\(\theta \in S\)</span> and <span class="math inline">\(\theta&#39; \in C\)</span> such that <span class="math inline">\(\|\theta - \theta&#39;\|_2 \leq \varepsilon\)</span>. The first and second terms are bounded by <span class="math inline">\(L \|\theta - \theta&#39;\|_2 \leq L\varepsilon\)</span> due to <span class="math inline">\(L\)</span>-lipschitzness of <span class="math inline">\(R\)</span> and <span class="math inline">\(\hat R\)</span>. (Note: these bounds is true wp 1, since there <em>always</em> exists such a <span class="math inline">\(\theta&#39;\)</span> by definition of <span class="math inline">\(\varepsilon\)</span>-net). It also follows from Solution  that the second term is upper bounded by <span class="math inline">\(\alpha\)</span>. We may take this to be true wp 1 because we condition on it. Therefore, by Equation <a href="#eq:triangle" data-reference-type="ref" data-reference="eq:triangle">[eq:triangle]</a>, <span class="math display">\[|\hat{R}(h_\theta) - R(h_\theta)| \leq L\varepsilon + \alpha + L\varepsilon = 2L\varepsilon + \alpha.\]</span> for any <span class="math inline">\(\theta \in S\)</span>.</p></li>
<li><p>(<span>3pts</span>)</p>
<p><span id="sol:2f" label="sol:2f">[sol:2f]</span> Recall the result from Solution : <span class="math display">\[|\hat{R}(h_\theta) - R(h_\theta)| \leq L\varepsilon + \alpha + L\varepsilon = 2L\varepsilon + \alpha, \hspace{2 em} \forall \theta \in S\]</span> Simply substituting values for <span class="math inline">\(\alpha, \varepsilon\)</span>, we get <span class="math display">\[|\hat{R}(h_\theta) - R(h_\theta)| \leq L\alpha + \alpha = 10 \sqrt {\frac{d \log n}{n}}(L + 1) = 20 \sqrt{\frac{d\log n}{n}},\]</span> as desired, where the first inequality comes from conditioning on <span class="math inline">\(E\)</span>. To show the probability bound, we return to the derivation of <span class="math inline">\(E\)</span> in Solution  with <span class="math inline">\(\gamma = 20 \sqrt{\frac{d\log n}{n}} = 4 \varepsilon\)</span>: <span class="math display">\[\begin{aligned}
            \Pr(E) &amp;\geq 1 - 2|C| \exp\left(-\frac{2 n \gamma^2}{M^2}\right)\\ 
            &amp;\geq 1 - 2 (3\sqrt d/ \varepsilon)^d  \exp\left(-2n(4 \varepsilon)^2\right) \\
            &amp;= 1 - \exp\left(-32n \varepsilon^2 + d \log 6 \sqrt d / \varepsilon\right) \\
            &amp;= 1 - \exp\left(-32 \cdot 25 d \log n + d \log (6/5 \sqrt{n/\log n})\right) \\
            &amp;= 1 - \exp(-d(800\log n - \log (6/5 \sqrt{n/\log n}))) \\
            &amp;\geq 1-e^{-d}
        \end{aligned}\]</span> where <span class="math inline">\(800 \log n - \log (6/5 \sqrt{n / \log n}) &gt; 1\)</span> when <span class="math inline">\(n &gt; 1\)</span> and does not depend on <span class="math inline">\(d\)</span>.</p></li>
</ol>
<h1 class="unnumbered" id="problem-3-vc-dimension">Problem 3: VC dimension</h1>
<ol type="a">
<li><ol type="i">
<li><p>(<span>3pts</span>)</p>
<p><span id="sol:3ai" label="sol:3ai">[sol:3ai]</span> Note that there is a bijection: <span class="math display">\[\{f_{\theta,b}(x) =  {\text{\rm sign}}\left(\left\langle x,\theta \right\rangle+b\right) : \theta \in \mathbb{R}^d, b \in \mathbb{R}\} \leftrightarrow \{f_{\theta&#39;}(x&#39;) =  {\text{\rm sign}}\left(\left\langle x&#39;,\theta&#39; \right\rangle\right) : \theta&#39; \in \mathbb{R}^{d+1}\},\]</span> the nonhomeogenous halfspaces in <span class="math inline">\(\mathbb{R}^d\)</span> are equivalent to homogenous halfspaces in <span class="math inline">\(\mathbb{R}^{d+1}\)</span>, <span class="math inline">\({\cal H}_{S, d+1}\)</span>. (This is clear by letting <span class="math inline">\(x&#39; = \langle x_1, x_2, ..., x_d, b\rangle\)</span> and <span class="math inline">\(\theta&#39; = \langle \theta_1, \theta_2, ..., \theta_d, 1 \rangle\)</span>). So we are done if we show the construction against <span class="math inline">\({{\mathcal{H}}}_{S, d+1}\)</span>.<br />
We claim, in fact, that for <em>any</em> basis of <span class="math inline">\(\mathbb{R}^{d+1}\)</span> (<span class="math inline">\(\{{\bf x}_1, ..., {\bf x}_{d+1}\}\)</span> that span <span class="math inline">\(\mathbb{R}^{d+1}\)</span>) and any given labeling <span class="math inline">\(\{y_1, ... y_{d+1}\} \in \{ \pm 1\}^{d+1}\)</span>, there exists <span class="math inline">\(f \in {\cal H}_{S, d+1}\)</span> such that <span class="math inline">\(f(x_i) = y_i\)</span>. To do this, we ignore the <span class="math inline">\({\text{\rm sign}}\)</span> and show directly <span class="math inline">\(\exists \; \theta&#39;\in \mathbb{R}^{d+1}\)</span> where <span class="math inline">\(\langle \theta&#39;, {\bf x}_i \rangle = y_i\)</span>. First, we define <span class="math display">\[X := \begin{pmatrix} {\bf x}_1 \\ {\bf x}_2 \\ \vdots \\{\bf x}_{d+1} \end{pmatrix} \in \mathbb{R}^{(d+1) \times (d+1)}, Y := \begin{pmatrix} y_1 \\ y_2 \\ \vdots \\y_{d+1} \end{pmatrix} \in \{ \pm 1\}^{d+1}, \theta&#39; := \begin{pmatrix} \theta&#39;_1 \\ \theta&#39;_2 \\ \vdots \\\theta&#39;_{d+1} \end{pmatrix} \in \mathbb{R}^{d+1}.\]</span> We seek a consistent solution to <span class="math display">\[X\theta&#39; =Y.\]</span> Since <span class="math inline">\(X\)</span> is a square matrix with linearly independent row-vectors (by definition of basis), we have that <span class="math inline">\(X^{-1}\)</span> exists and thus <span class="math inline">\(\theta&#39; = X^{-1}Y\)</span> exists by <a href="https://en.wikipedia.org/wiki/Invertible_matrix">Invertible Matrix Theorem</a> and we are done.</p></li>
<li><p>(<span>4pts</span>)</p>
<p>First, recognize the bijection of Solution  that allows us to show impossibility for the homogeneous problem in <span class="math inline">\(\mathbb{R}^{d+1}\)</span> using <span class="math inline">\({\cal H}_{S, d+1}\)</span>. So, it remains to decide labelings <span class="math inline">\(\{y_1, y_2, ..., y_{d+2}\}\)</span> of (arbitrary) <span class="math inline">\(\mathbb{R}^{d+1}\)</span> vectors <span class="math inline">\(\{{\bf x}_1, {\bf x}_2, ..., {\bf x}_{d+2} \}\)</span> such that it is <em>impossible</em> to produce <span class="math inline">\(\mathbf{w}\in \mathbb{R}^{d+1}\)</span> where <span class="math inline">\(\langle \mathbf{w}, \mathbf{x}_i \rangle = y_i\)</span> always. The idea is to take advantage of the strictness of the <span class="math inline">\({\text{\rm sign}}\)</span> operator. So, we propose <span class="math inline">\(y_i = {\text{\rm sign}}(c_i)\)</span> where <span class="math inline">\(c_i\)</span> satisfy <span class="math display">\[\sum_{i =1}^{d+2} c_i {\bf x}_i = 0.\]</span> Since <span class="math inline">\(d+2\)</span> points in <span class="math inline">\(d+1\)</span>-dimensional space contain <em>at least</em> one pair that are linearly dependent, we know that there is a solution <span class="math inline">\({\bf c} \neq 0\)</span> by Invertible Matrix Theorem. (If there wasn’t, there would be a unique solution to the system, which happens if and only if <span class="math inline">\(X\)</span> is square and has linearly independent rows – this is not the case). Let <span class="math inline">\(C^+ = \{ i : c_i &gt; 0\}\)</span> and <span class="math inline">\(C^- = \{ j : c_j &lt; 0\}\)</span>. Then, <span class="math display">\[\sum_{i} c_i {\bf x}_i = \sum_{i \in C^+} c_i {\bf x}_i - \sum_{j \in C^-} |c_j|{\bf x}_j = 0 \Rightarrow \sum_{i \in C^+} c_i {\bf x}_i = \sum_{j \in C^-} |c_j|{\bf x}_j \label{eq:split}\]</span> Now, we seek <span class="math inline">\(\mathbf{w}\)</span> where <span class="math inline">\(\langle \mathbf{w}, \mathbf{x}_i \rangle &gt; 0\)</span> for <span class="math inline">\(i \in C^+\)</span> and <span class="math inline">\(\langle \mathbf{w}, \mathbf{x}_j \rangle &lt; 0\)</span> for <span class="math inline">\(j \in C^+\)</span>, always. But, we see the equality of Equation <a href="#eq:split" data-reference-type="ref" data-reference="eq:split">[eq:split]</a> makes this strictly impossible, since <span class="math display">\[\left\langle\sum_{i \in S^+} c_i {\bf x}_i, {\bf w}\right\rangle = \left\langle\sum_{j \in S^-} |c_j| {\bf x}_j, {\bf w}\right\rangle \Rightarrow c_i \left\langle {\bf x}_i, {\bf w}\right\rangle = |c_j|\left\langle {\bf x}_j, {\bf w} \right\rangle \label{eq:contradiction}\]</span> has LHS strictly less than 0, and RHS strictly greater! Therefore, such <span class="math inline">\(\bf w\)</span> never exists, and our labeling cannot be shattered by <span class="math inline">\({\cal H}_{S, d+1}\)</span>.<br />
(Remark: It is possible for <span class="math inline">\(|C^+|\)</span> or <span class="math inline">\(|C^-|\)</span> to be empty. In this case, the RHS or LHS of Equation <a href="#eq:contradiction" data-reference-type="ref" data-reference="eq:contradiction">[eq:contradiction]</a> becomes exactly 0, and we reach a contradiction again – the LHS or RHS is strictly larger or smaller, respectively).</p></li>
</ol></li>
<li><p>The setup describes an <span class="math inline">\(M\)</span> layer feedforward neural network <span class="math inline">\(h : \{\pm 1\}^{r_1} \rightarrow \{\pm 1\}\)</span>, with layer <span class="math inline">\(i\)</span> containing <span class="math inline">\(r_i\)</span> nodes each with value in <span class="math inline">\(\{ \pm 1\}\)</span>. The function class <span class="math inline">\({\mathcal{F}}_k\)</span> contains all mappings from layer <span class="math inline">\(r_k\)</span> to <span class="math inline">\(r_{k+1}\)</span> (that is, mappings from <span class="math inline">\(\{ \pm 1\}^{r_k}\)</span> to <span class="math inline">\(\{ \pm 1\}^{r_{k+1}}\)</span>). The network class <span class="math inline">\({\mathcal{H}}\)</span> is the composition of the results from each of the layers to a final <span class="math inline">\((M+1)\)</span>-th layer with single result <span class="math inline">\(\{ \pm 1\}\)</span>.</p>
<ol type="i">
<li><p>(<span>4pts</span>)</p>
<p><span id="sol:3bi" label="sol:3bi">[sol:3bi]</span> On inspection, it’s clear that we need to show the following two statements hold for arbitrary function classes <span class="math inline">\({\mathcal{C}}_1, {\mathcal{C}}_2\)</span>: <span class="math display">\[\Pi_{{\mathcal{C}}_1 \times {\mathcal{C}}_2}(n) \leq \Pi_{{\mathcal{C}}_1}(n) \Pi_{{\mathcal{C}}_2}(n)\label{eq:3bi1},\]</span> <span class="math display">\[\Pi_{{\mathcal{C}}_1 \circ \; {\mathcal{C}}_2}(n) \leq \Pi_{{\mathcal{C}}_1}(n) \Pi_{{\mathcal{C}}_2}(n)\label{eq:3bi2}.\]</span> If we show Equations <a href="#eq:3bi1" data-reference-type="ref" data-reference="eq:3bi1">[eq:3bi1]</a> and <a href="#eq:3bi2" data-reference-type="ref" data-reference="eq:3bi2">[eq:3bi2]</a>, we can inductively extend it to <span class="math inline">\({\mathcal{C}}_1 \times \cdots \times {\mathcal{C}}_M\)</span> and <span class="math inline">\({\mathcal{C}}_1 \circ \cdots \circ {\mathcal{C}}_M\)</span> where <span class="math inline">\(M &lt; \infty\)</span> to get the same product relationship. Then, for function class <span class="math inline">\({\mathcal{F}}_k\)</span> representing layer <span class="math inline">\(k\)</span>, we have <span class="math display">\[\Pi_{{\mathcal{F}}_k}(n) \leq \prod_{i=1}^{r_{k+1}} \Pi_{{\mathcal{H}}_k}(n) = (\Pi_{{\mathcal{H}}_k}(n))^{r_{k+1}},\]</span> since <span class="math inline">\({\mathcal{F}}_k = {\mathcal{H}}_1 \times \cdots \times {\mathcal{H}}_{r_{k+1}}\)</span> and Equation <a href="#eq:3bi1" data-reference-type="ref" data-reference="eq:3bi1">[eq:3bi1]</a>.<br />
The growth function of the entire network would then be <span class="math display">\[\Pi_{{\mathcal{H}}}(n) \leq \prod_{k=1}^M \Pi_{{\mathcal{F}}_k}(n) \leq \prod_{k=1}^M \left(\Pi_{{\mathcal{H}}_k}(n)\right)^{r_{k+1}},\]</span> since <span class="math inline">\({\mathcal{H}}= {\mathcal{F}}_1 \circ \cdots \circ {\mathcal{F}}_M\)</span> and Equation <a href="#eq:3bi2" data-reference-type="ref" data-reference="eq:3bi2">[eq:3bi2]</a>.<br />
</p>
<hr />
<p>We attack Equation <a href="#eq:3bi1" data-reference-type="ref" data-reference="eq:3bi1">[eq:3bi1]</a> first. Let <span class="math inline">\(S = \{ {\mathbf{x}}_1, ..., {\mathbf{x}}_n\}\)</span>, define the restriction of <span class="math inline">\({\mathcal{C}}\)</span> to <span class="math inline">\(S\)</span> as <span class="math display">\[|{\mathcal{C}}_S| = |\{ (c({\mathbf{x}}_1), ... c({\mathbf{x}}_n)) : c \in {\mathcal{C}}\}|.\]</span> Note that <span class="math display">\[\begin{aligned}
        |({\mathcal{C}}_1 \times {\mathcal{C}}_2)_S| = |\{((c_1({\mathbf{x}}_1), c_2({\mathbf{x}}_2)), ..., (c_1({\mathbf{x}}_n), c_2({\mathbf{x}}_n))) : c_1 \in {\mathcal{C}}_1, c_2 \in {\mathcal{C}}_2\}|,
    \end{aligned}\]</span> by definition, where we have <span class="math inline">\(n\)</span> pairs such that the first function is from <span class="math inline">\({\mathcal{C}}_1\)</span> and the second from <span class="math inline">\({\mathcal{C}}_2\)</span>. We suggest that the size of this set is the same as that of a pair of <span class="math inline">\(n\)</span>-tuples arranged by grouping elements from <span class="math inline">\({\mathcal{C}}_1\)</span> and <span class="math inline">\({\mathcal{C}}_2\)</span>: <span class="math display">\[|({\mathcal{C}}_1 \times {\mathcal{C}}_2)_S| = |\{((c_1({\mathbf{x}}_1), ..., c_1({\mathbf{x}}_n)), (c_2({\mathbf{x}}_1), ..., c_2({\mathbf{x}}_n))) : c_1\in {\mathcal{C}}_1, c_2 \in {\mathcal{C}}_2\}|,\label{eq:3bi3}\]</span> because there is an easy bijection between the two representations. Note that the two groupings in the representation of Equation <a href="#eq:3bi3" data-reference-type="ref" data-reference="eq:3bi3">[eq:3bi3]</a> are <em>disjoint</em>, therefore we can separate them into a Cartesian product: <span class="math display">\[\begin{aligned}
        |({\mathcal{C}}_1 \times {\mathcal{C}}_2)_S| &amp;= \text{RHS of Equation~\ref{eq:3bi3}} = |({\mathcal{C}}_1)_S \times ({\mathcal{C}}_2)_S| \\
        &amp;= |\{ (c_1({\mathbf{x}}_1), ..., c_1({\mathbf{x}}_n)) : c_1 \in {\mathcal{C}}_1 \}| \cdot |\{ (c_2({\mathbf{x}}_1), ..., c_2({\mathbf{x}}_n)) : c_2 \in {\mathcal{C}}_2\}| \\
        &amp;= |({\mathcal{C}}_1)_S| |({\mathcal{C}}_2)_S|.
    \end{aligned}\]</span> From the definition of <span class="math inline">\(\Pi_{\mathcal{C}}(m) = \max_{S : |S| = m} |{\mathcal{C}}_S|\)</span>, we have <span class="math display">\[\begin{aligned}
        \Pi_{{\mathcal{C}}_1 \times {\mathcal{C}}_2} (m) &amp;= \max_{S : |S| = m} |({\mathcal{C}}_1 \times {\mathcal{C}}_2)_S)| = \max_{S : |S| = m} |({\mathcal{C}}_1)_S| |({\mathcal{C}}_2)_S| \\
        &amp;\leq \max_{S : |S| = m} |({\mathcal{C}}_1)_S| \max_{S : |S| = m}|({\mathcal{C}}_2)_S| = \Pi_{{\mathcal{C}}_1}(m) \Pi_{{\mathcal{C}}_2}(m),
    \end{aligned}\]</span> which is exactly Equation <a href="#eq:3bi1" data-reference-type="ref" data-reference="eq:3bi1">[eq:3bi1]</a>. We try the same decomposition for <span class="math inline">\({\mathcal{C}}_1 \circ {\mathcal{C}}_2\)</span>: <span class="math display">\[\begin{aligned}
        |({\mathcal{C}}_1 \circ {\mathcal{C}}_2)_S| &amp;= |\{(c_1(c_2({\mathbf{x}}_1)), ..., c_1(c_2({\mathbf{x}}_n))): c_1 \in {\mathcal{C}}_1, c_2 \in {\mathcal{C}}_2\}|, \\
        &amp;= \left|\bigcup_{f \in {\mathcal{C}}_1}\left\{ (f(c_2({\mathbf{x}}_1)), ..., f(c_2({\mathbf{x}}_n))) : c_2 \in {\mathcal{C}}_2 \right\}\right| \\
        &amp;\leq |({\mathcal{C}}_1)_S| |({\mathcal{C}}_2)_S|.
    \end{aligned}\]</span> The tuple <span class="math inline">\((c_2({\mathbf{x}}_1), ..., c_2({\mathbf{x}}_n))\)</span> can take on at most <span class="math inline">\(({\mathcal{C}}_2)_S\)</span> values by definition, and for each position <span class="math inline">\(i \in [n]\)</span>, <span class="math inline">\({\mathcal{C}}_1\)</span> can represent at most <span class="math inline">\(({\mathcal{C}}_1)_S\)</span> values through <span class="math inline">\(f(c_2({\mathbf{x}}_i))\)</span>. This justifies the final conclusion <span class="math display">\[\begin{aligned}
        \Pi_{{\mathcal{C}}_1 \circ \; {\mathcal{C}}_2} (m) &amp;= \max_{S : |S| = m} |({\mathcal{C}}_1 \circ \; {\mathcal{C}}_2)_S)| \leq \max_{S : |S| = m} |({\mathcal{C}}_1)_S| |({\mathcal{C}}_2)_S| \\
        &amp;\leq \max_{S : |S| = m} |({\mathcal{C}}_1)_S| \max_{S : |S| = m}|({\mathcal{C}}_2)_S| = \Pi_{{\mathcal{C}}_1}(m) \Pi_{{\mathcal{C}}_2}(m),
    \end{aligned}\]</span> and we are done.</p></li>
<li><p>(<span>2pts</span>)</p>
<p>We take logarithms of the result from <a href="#sol:3bi" data-reference-type="ref" data-reference="sol:3bi">[sol:3bi]</a>: <span class="math display">\[\log (\Pi_{\mathcal{H}}(n)) \leq \sum_{i = 1}^M \log(\left(\Pi_{{\mathcal{H}}_k}(n)\right)^{r_{k+1}}) = \sum_{i = 1}^M r_{k+1}\log(\Pi_{{\mathcal{H}}_k}(n)). \label{eq:3bii1}\]</span> Here, we recall Sauer’s Lemma, which gives polynomial upper bound for <span class="math inline">\(\Pi_{{\mathcal{H}}_k}(m) \leq (em/d_k)^{d_k}\)</span>, when <span class="math inline">\(m &gt; {\text{\rm VCdim}}({\mathcal{H}}_k) = d_k\)</span>. Since we are given <span class="math inline">\(n &gt; d+1\)</span> it is easy to see that <span class="math inline">\(\forall \; k\)</span>, <span class="math inline">\(n &gt; d_k\)</span> (by setting all <span class="math inline">\(r_i = 0\)</span> where <span class="math inline">\(i \neq k\)</span> and <span class="math inline">\(r_k=1\)</span> we trivially recover just <span class="math inline">\({\mathcal{H}}_k\)</span>). Therefore, we can use the lemma on every term in the sum and simplify Equation <a href="#eq:3bii1" data-reference-type="ref" data-reference="eq:3bii1">[eq:3bii1]</a>, <span class="math display">\[\begin{aligned}
    \log (\Pi_{\mathcal{H}}(n)) &amp;\leq \sum_{i = 1}^M r_{k+1} \log((en/d_k)^{d_k}) \\
    &amp;= \sum_{i =1 }^M r_{k+1} d_k (\log (en) - \log (d_k) ) \\
    &amp;\leq \sum_{i =1 }^M r_{k+1} d_k (\log (en)) = d \log (en) \end{aligned}\]</span> by definition of <span class="math inline">\(d\)</span>. After the hard work is done, we exponentiate to get the result: <span class="math display">\[\Pi_{\mathcal{H}}(n) \leq e^{d \log (en)} = e^{\log ((en)^d)} = (en)^d.\]</span></p></li>
<li><p>(<span>3pts</span>)</p>
<p>Notice that the growth function derived in part bii is <span class="math inline">\(\mathop{\mathrm{poly}}(n)\)</span>, though it initially grows faster than <span class="math inline">\(2^n\)</span>, it will eventually be overtaken. The value <span class="math inline">\(m\)</span> at which this happens is the VC dimension, because <span class="math inline">\({\mathcal{H}}\)</span> can no longer shatter <em>any</em> set of that size. Therefore, we seek the largest value where the growth function is ahead of complete shattering <span class="math inline">\(2^n\)</span>, or <span class="math display">\[(en)^d \geq 2^n \Rightarrow n \leq d (1 + \log n) \label{eq:3biii1}\]</span> The only issue is that we have <span class="math inline">\(n\)</span> terms on both sides of the inequality, without <span class="math inline">\(\log d\)</span>. To resolve this, we first play with the inequality <span class="math display">\[n \leq d \log n \Rightarrow \log n \leq \log d + \log \log n  \Rightarrow \ensuremath{\mathcal{O}}(\log n) \leq \ensuremath{\mathcal{O}}(\log d)\]</span> where we’ve used the fact that <span class="math inline">\(\log n - \log \log n = \Theta (\log n)\)</span>. We use this into RHS of Equation <a href="#eq:3biii1" data-reference-type="ref" data-reference="eq:3biii1">[eq:3biii1]</a>, <span class="math display">\[\begin{aligned}
        n \leq d + d \log n \leq d + \ensuremath{\mathcal{O}}(d \log d) = \ensuremath{\mathcal{O}}(d \log d),
    \end{aligned}\]</span> and we are done. The largest value of <span class="math inline">\(n\)</span> that satisfies the ‘complete shattering’ inequality is bounded by <span class="math inline">\(\ensuremath{\mathcal{O}}(d \log d)\)</span> as desired.<br />
</p></li>
</ol></li>
<li><p>(<span>4pts</span>)</p>
<p>Notice that <span class="math display">\[\theta = \pi(1+ \sum_{i=1}^n (1-y_i)2^{2i-1}) = \pi(1 + \sum_{i=1}^n (1-y_i)\frac{1}{2x_i} )\]</span> using the definition that <span class="math inline">\(x_i = 2^{-2i}\)</span>. We also introduce notation <span class="math inline">\(I^+ = \{ i : y_i = 1\}\)</span> and <span class="math inline">\(I^- = \{ i : y_i = -1\}\)</span>. Notice that, for any <span class="math inline">\(i \in I^+\)</span>, we have <span class="math inline">\((1-y_i) = 0\)</span>, so the quantity <span class="math inline">\((1-y_i)\frac{1}{2x_i} = 0\)</span> as well. For any <span class="math inline">\(j \in I^-\)</span>, we have <span class="math inline">\((1-y_i)\frac{1}{2x_i} = \frac{1}{x_i}\)</span>. Therefore, the expression for <span class="math inline">\(\theta\)</span> is equivalent to <span class="math display">\[\theta = \pi \left(1 + \sum_{j \in I^-} \frac{1}{x_j} \right).\]</span> We remark that the summation is an even integer because <span class="math inline">\(1/x_j = 2^{2j}\)</span> is even for every term.</p>
<p><span class="math inline">\((\underline{1})\)</span>: Now, consider the value <span class="math inline">\(\theta x_k\)</span> where <span class="math inline">\(k \in I^+\)</span>: <span class="math display">\[\begin{aligned}
    \theta x_k &amp;= \pi \left(x_k + \sum_{j \in I^-} \frac{x_k}{x_j}\right) = \pi x_k + \pi \sum_{j \in I^-} 2^{2(j-k)} \\
    &amp;= \pi \left(x_k + \sum_{j \in I^-,\; k &lt; j } 2^{2(j-k)} + \sum_{j \in I^-,\; k &gt; j } 2^{2(j-k)}\right),\end{aligned}\]</span> where we have broken up indices in <span class="math inline">\(I^-\)</span> into two sets, one larger than <span class="math inline">\(k\)</span> and the other smaller. We’re not particularly concerned about the former, because all terms are even integers and so is the sum. The latter term is upper-bounded by <span class="math display">\[\sum_{j \in I^-,\; k &gt; j } 2^{2(j-k)} \leq 2^{2(-1)} + 2^{2(-2)} + \cdots = \frac{2^{-2}}{1 - 1/4} = \frac13,\]</span> so we have <span class="math display">\[\theta x_k \leq \pi (x_k + 2c + 1/3) \leq \pi (1/4 + 1/3) + 2c \pi = 7 \pi /12 + 2c \pi,\]</span> where we’ve used the fact that <span class="math inline">\(x_k \leq x_1 = 1/4\)</span>. Clearly, <span class="math inline">\(\sin(7 \pi /12 + 2c \pi) = \sin (7 \pi/12) &gt; 0\)</span>, so <span class="math inline">\({\text{\rm sign}}(\sin(\theta(x_k))) = 1\)</span>, as expected.</p>
<p><span class="math inline">\((\underline{2})\)</span>: Now, consider the value <span class="math inline">\(\theta x_k\)</span> where <span class="math inline">\(k \in I^-\)</span>: <span class="math display">\[\begin{aligned}
    \theta x_k &amp;= \pi \left(x_k + \sum_{j \in I^-} \frac{x_k}{x_j}\right) = \pi x_k + \pi \sum_{j \in I^-} 2^{2(j-k)} \\
    &amp;= \pi \left(x_k + \sum_{j \in I^-,\; k &lt; j } 2^{2(j-k)} + 
    \sum_{j \in I^-,\; k = j } 2^{2(j-k)} + \sum_{j \in I^-,\; k &gt; j } 2^{2(j-k)}\right) \\
    &amp;= \pi \left(x_k + \sum_{j \in I^-,\; k &lt; j } 2^{2(j-k)} + 
    1  + \sum_{j \in I^-,\; k &gt; j } 2^{2(j-k)}\right),\end{aligned}\]</span> using the fact that <span class="math inline">\(k \in I^-\)</span> (exactly once). We note again that the first summation is a sum of even integers and is representable as <span class="math inline">\(2d &gt; 0\)</span>, and the second summation is bounded above by <span class="math inline">\(1/3\)</span>. So, <span class="math display">\[\theta x_k \leq \pi (x_k + 2d + 1 +  1/3) \leq \pi (1/4 + 1/3 + 1) + 2d \pi = 19 \pi/12 + 2c \pi.\]</span> Clearly, <span class="math inline">\(\sin(19\pi/12 + 2d \pi) = \sin(19\pi/12) &lt; 0\)</span>, so <span class="math inline">\({\text{\rm sign}}(\sin(\theta x_k)) = -1\)</span> in this case, as expected.</p>
<p>Therefore, for any set of size <span class="math inline">\(n\)</span> as <span class="math inline">\(C = \{ x_1, ..., x_n : \; x_i = 2^{-2i}, i \in [n] \}\)</span>, we have shown that the function <span class="math display">\[f \in {\mathcal{H}}, f = {\text{\rm sign}}\left(\sin\left(\pi\left(1 + \sum_{i=1}^n (1-y_i)2^{2i-1}x\right)\right)\right)\]</span> shatters <span class="math inline">\(C\)</span>, thus <span class="math inline">\({\text{\rm VCdim}}({\mathcal{H}}) = \infty\)</span>.</p></li>
</ol>
